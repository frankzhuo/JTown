自研中间件技术作为一切的支撑，并为后续架构演进打下基础。

双11要解决的问题：
    较低的成本、最大的吞吐量、最佳的用户体验，三者取得平衡

愿景：
    精细化、数据化、智能化

机房部署方式进化：
    1.0时代：超级机房
        同城容灾、异地冷备
            问题：
                对机房规模要求变大，单个机房内机器数量巨大
                负载均衡模式下，机房间调用的连接数问题也是个挑战
                数据库将无法承受如此巨大的连接数消耗
                IDC限制
                单城市无法支撑我们的业务体量
                国际化部署问题
                超远距离跨区域调用延迟（每公里多1ms、延迟问题会随着节点距离、数量的增加变得越来越不可控）

    2.0时代：单元机房
        在单元内实现业务闭环
            优势：
                解决1.0时代的所有问题
                可扩展
                可收缩
            支撑：
                快速切量
                    中间件支撑
                数据同步
                    实时的把买家在这个单元产生的数据同步到中心，卖家数据要同步到单元，都要实时做
                    比如说发现1秒的延时，要等数据同步完成才能切换过来
                    数据同步效率非常关键

如何评估容量：
    线上压测
        线上压测的手段：
            用自研中间件技术进行线上引流，将所有流量引流到待压测的10%的节点上，观察流量、节点负载、响应时间的变化关系
    全链路压测技术
        优势：
            可以发现全链路的问题：业务技术瓶颈、容量调配、性能问题、隐藏bug
            及时暴露（致命）问题，指导后续容量调整、瓶颈消除、问题修复、性能优化
        方法论：
            并发的问题只有在大流量下才会暴露，小流量的线下测试、受限场景，是验不出来的

如何为双11流量峰值提前做准备？
    提前发现问题。
        手段：
            平时模拟双11流量？
                如何模拟？
                    在线上用大规模的流量去压测。
                    从IDC到网络、中间件、应用、缓存、数据库，所有基础设施能够用同样的流量模型来验证，
                    这也要求在线上做，而且要做海量请求，跟双11同样的流量验证整个集群的实际能力。
                    自研的分布式流量引擎，部署在全国的CDN上，深度定制，以精确模拟双11流量，从全国各地发起压力。
                    只压测双11最重要的业务场景

                为何要在线压测？
                    因为必须要用真实数据，才能准确还原双11真实的计算资源消耗
    
    方法论：
        化被动为主动
        应急措施往往很被动
        应急出现的问题往往预期不到，没有很好的手段去解决。临时写的应急脚本执行后可能会带来更大的问题
        要尽量减少应急的情况，尽量做到全都是确定的


稳定性的基础：
    压测演练、技术支撑、一切可控、减少应急情况

成本优化：
    通过性能优化降低成本：
        单一系统优化确实会局部有所提升，但未必能对全局提升有过多帮助：胡子眉毛一把抓，500个系统都优化20%，最后不一定是整体水平提升20%
        性能优化的关键还是在于找到瓶颈，对瓶颈进行优化，这个效果是最好的
        解决瓶颈问题、架构升级，是提升整体水平的有效手段
    通过谈性计算降低成本：        
        一年只有一天是超高值，其余都是低水平
        混合云弹性架构，按需使用资源，用完则归还
        结合2.0单元机房使用：（人工加自动化运维手段）资源调匀、全链路压测
    提升效率：
        中间件技术支撑
        docker化

运行控制：
    限流保护、有损服务 -> 业务自动降级
        系统容量已经调匀，有了最大吞吐能力，但是用户热情很夸张，超出想象。
        超出整个集群处理能力的流量，需要把它挡在外面，如果放进来的话，整个集群就一起宕掉了，一个请求都处理不了，
        整体瘫痪，这时必须做限流保护和有损服务，放进来的流量好好处理，没有放进来的，等一会儿刷一下也会进来，这是通用做法。
        
        原则：
            处理不了就别放进来，放进来的就是处理得了的，外面入口收紧一点，里面处理放松一点，有很多自我保护手段

    流量调度
        将集群中负载过大的机器降权，当负载恢复正常后恢复正常优先级


开关和预案：
    完备的预案体系
    一系列的开关可以有序、完整地被执行，保证一致性、完整性
    经验教训：双11的时候千万别执行没有测试过的开关，因为往往会出问题

稳定性治理
    业务复杂到一定程度之后，人工已无能为力，只能用架构、技术手段去识别架构复杂度、理清关系
    整理出依赖关系，对海量调用进行统计，得到各个系统的稳定性指标，这都是从数据中挖掘的。
    比如说调用层次、响应时间，哪里阻断了会影响业务，哪里阻断了不会影响业务，都是被挖掘出来的。
    同时会结合业务测试用例看哪些链条可以被降级，哪些不可以降级，有些降下来主业务就不显示了。
    技术同学会对弱依赖做业务容错，强依赖要保证系统不能挂，稳定性治理非常关键。

监控&补偿：
    每个大型业务系统的必备运维单元
        